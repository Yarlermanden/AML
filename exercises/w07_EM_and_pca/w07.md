# Exercise week 7

This is your 7th mandatory exercise which must be completed until next week, 25.10.2022, 14:00. By that time you must fill out the checklist on the learnit page to indicate which tasks you completed and volunteer to present.

## Task 1: EM Algorithm via kmeans
Assume data points $\mathbf{x}\_n\in\mathbb{R}^D$ are given and you know the data is separated into $K$ clusters. Each cluster is represented by one cluster center $\mathbf{m}_k\in\mathbb{R}^{D}$, hence all of them can be gathered in the following matrix 
$$\mathbf{M} := [\mathbf{m}_1,\ldots, \mathbf{m}_K ] \in \mathbb{R}^{D\times K}. $$

Each point $\mathbf{x}\_n \in \mathbb{R}^D$ is assiged to one cluster. 

$$r_{nk} := \begin{cases} 
1 & , ~ \mathbf{x}\_n \text{ belongs to cluster }k\\\ 
0 & , ~ \text{otherwise} \end{cases}
$$ 

$$\mathbf{R} := \begin{pmatrix}  r_{11} & \ldots & r_{1K} \\\ \vdots && \vdots \\\ r_{N1} & \ldots & r_{NK} 
\end{pmatrix}\in\mathbb{R}^{N\times K},$$ 

where $r_{nk}\in \{0,1 \}$ are an indicator variables, which are one if $\mathbf{x}\_n$ belongs to cluster $k$.  

The goal is to 
<ol>
  <li> Estimate the cluster centers, i.e.  $\mathbf{m}_k \in \mathbb{R}^D$, and </li>
  <li> for each data point $\mathbf{x}_n$ assign one cluster center per point. </li>	
</ol>

Therefore, we aim to minimize:

$$J(\mathbf{M},\mathbf{R}) = \sum_{n=1}^N\sum_{k=1}^K r_{nk} ||\mathbf{x}\_n-\mathbf{m}_k||_2^2. $$

In the EM-algorithm the estimation of the unknowns $\mathbf{m}\_{k}$ and $r_{nk}$ is alternated, by fixing one of them while estimating the other. 

In the following compute the derivative with respect to one variable, i.e. $r_{nk}$ and $\mathbf{m}_{k}$. 

<ol type ="a">
  <li> Assume $\mathbf{ m }_k $ are known, estimate the binary values $r_{nk}$. Keep in mind that for each $n$ one $k$ must be assigned. 	  
	$$ J(\mathbf{R}|\mathbf{M})=\sum\limits_{n=1}^N  g(r_{nk}) $$
	  $$ g( r_{ nk } ) = \sum\limits_{k=1}^K r_{nk} ||\mathbf{x}_n-\mathbf{m}_k||_2^2 $$  
Hints:
<ul>
 <li> Consider that this can be done independently for each single data point $\mathbf{x}_n$. </li>
 <li> When you compute the derivative and set it to zero, you might end up confused. Consider this: At some point, do not calculate but give a reason for a good choice of the binary values $r_{kn}$ in written form, instead.  </li>
 </ul>
  </li>
	
   <li> Assume $r_{nk}$ are known, estimate $\mathbf{m}_k$. Compute the derivative of $J$ with respect to $\mathbf{m}_k$, set it to zero and then deduce the estimates for $\mathbf{m}_k$. By rewriting 
	$$ J(\mathbf{M}|\mathbf{R})= \sum_{k=1}^K  h(\mathbf{m}_k),$$
	we can focus on one cluster center $k$ at a time by:
	$$h(\mathbf{m}_k) = \sum\limits_{n=1}^N r_{nk} ||\mathbf{x}_n-\mathbf{m}_k||_2^2$$
	Hint: $||\mathbf{x}_n-\mathbf{m}_k||_2^2 = (\mathbf{x}_n-\mathbf{m}_k)^T (\mathbf{x}_n-\mathbf{m}_k)$
   </li>
</ol>



## Task 2: EM and GMMs
We provide the third party python notebook w07_Gaussian_Mixture_Models.ipynb for you to understand the Expectation Maximization for GMMs better. 
Test different parameters from the documentation:
<ol type ="a">
  <li>Try if you can find a good clustering for three clusters with GMMs.</li> 
  <li>How many iterations are needed? What is your best guess for the minimum number of iterations needed for a good result?</li>
  <li>Use different covariance types. How do the results differ?</li>
  <li>For data generation: Look into the section "GMM for Generating New Data". Describe how the generated digits look to you.</li>	
  <li>From previous task: Change the amount of training data and other parameters. How do your obserations from d change?</li>		
  <li>Extra non-mandatory: Change the data generation or use your own data and see how your findings change.</li>
</ol>


## Task 3: Principal Component Algorithm (PCA)

The main objective of this task it to implement the Principal Component Analysis (PCA) yourself. 
<ol type ="a">
  <li>The steps are described in detail in the following. To test your implementation of the steps (1)-(6) on the bodyfat data, which originally has $D=15$ dimensions. Use your PCA implementation to perform dimensionality reduction from $D=15$ to $M=2$. Plot the resulting 2D datapoints $\mathbf{z}_n\in \mathbb{R}^2$.</li> 
  <li>Perform dimensionality reduction via PCA using sklearn.decomposition.PCA. How do you have to set the input parameters?</li>
  <li>How can PCA be used to perform regression? </li>
</ol>

### PCA Algorithm - How to
Given: data points $\mathbf{x}_n \in\mathbb{R}^{D}$, $n=1,\ldots,N$.
<ol>
  <li> Compute sample mean $\overline{\mathbf{x}}=\frac{1}{N}\sum\limits_{n=1}^N \mathbf{x}_n$. 
  <li> Compute sample covariance matrix $\mathbf{S}=\frac{1}{N-1}\sum\limits_{n=1}^N (\mathbf{x}_n-\overline{\mathbf{x}})(\mathbf{x}_n-\overline{\mathbf{x}})^T$.
    Please note that you may find descriptions with a different factor $\frac{1}{N}$ vs. $\frac{1}{N-1}$, and other variants of PCA using the correlation matrix instead of covariance matrix. </li>
  <li> Compute eigenvalues and eigenvectors $\mathbf{S}\mathbf{u_i}=\lambda_i\mathbf{u_i}$, $i=1,\ldots,M$, $\mathbf{S}\in\mathbb{R}^{D\times D}$, $\mathbf{u}_i\in\mathbb{R}^D$.</li>
  <li> Reorder eigenvalues in decreasing order, i.e. $\lambda_i\geq \lambda_j$, $i>j$, and their eigenvectors correspondingly. </li>
  <li> Choose number of components $M < D$ as desired. Put the corresponding $M$ largest eigenvectors $\mathbf{u_i}$ in decreasing order (by eigenlvaue) into the matrix $\mathbf{U}=[\mathbf{u}_1,\ldots,\mathbf{u}_M]^T\in\mathbb{R}^{M \times D}$, i.e. such that $\mathbf{u}_1$ is the eigenvector corresponding to the largest eigenvalue. </li>
  <li> <b>How to feature extraction aka dimensionality reduction</b> from $\mathbf{x}\in\mathbb{R}^D$ to $\mathbf{z}\in\mathbb{R}^M$, $M < D$
    	$$ \mathbb{R}^{M} \ni \mathbf{z}_n = \mathbf{U}\left( \mathbf{x}_n-\overline{\mathbf{x}}\right). $$</li>
  <li> <b>How to create a new data point</b> $\mathbf{x}$. Choose (and vary) $\alpha_i$ between $-3\sqrt{\lambda_i} \leq \alpha_i \leq 3\sqrt{\lambda_i}$, to obtain
        $$  \mathbb{R}^{D} \ni \mathbf{x} = \overline{\mathbf{x}}+\sum_{i=1}^M \alpha_i \mathbf{u}_i .$$</li>
</ol>

## Task 4: PCA Face Model

The purpose of this task is to create a 3D face model from a set of 3D faces given as sets of 3D points. 
Use the provided data in faces.mat. 

Hint: To read mat-files in Python:
import scipy.io
mat = scipy.io.loadmat('file.mat')

<ol type ="a">
 <li> Estimate a PCA for the data matrix $X\in\mathbb{R}^{D\times N}$. Ideally you should use your own implementation from the previous task, but if you don't have one, use a buildin solution to proceed with the following tasks. Here, you are being provided with $N=40$ samples, i.e. 3D faces, stored columnwise. Each face has a data dimension of $D=339$, i.e. $113$ 3D points per 3D face.</li>
 <li> Plot the mean face $\overline{\mathbf{x}}\in \mathbb{R}^{D\times 1}$, and the variants of the first three principal components $\mathbf{u}_i \in\mathbb{R}^{D\times 1}$, $i=1,2,3$ as faces, i.e.: remember to add the mean face to each $\mathbf{u}_i$.</li> 
 <li> Create 3 new faces, which are not part of the data by chosing a fixed number of components $M$, and vary the weights $\alpha_k$. 
	$$\mathbb{R}^{D\times 1} \ni \mathbf{f} = \overline{\mathbf{x}}+\sum\limits_{k=1}^M \alpha_k \mathbf{u}_k .$$
   </li>
  <li> Approximate a new shape. Choose one of your created shapes from task c, and pretend it is unknown, i.e. pretend that the weights $\alpha_k$ are unknown. (This would be the case for every new shape, which is not part of the training data or has not been generated by you.) Then estimate the coefficients of the model with a lower dimension than before, i.e. choose $K < M$ and solve the following equation system for $\mathbf{\alpha}$: 
		$$\mathbb{R}^{D\times 1} \ni \mathbf{f}_\text{new}-\overline{\mathbf{x}} = \mathbf{U} \mathbf{\alpha} ,$$
  where $\mathbf{\alpha} = (\alpha_1,\ldots,\alpha_K)^T \in\mathbb{R}^K$, and $\mathbf{U}=[\mathbf{u_1},\ldots,\mathbf{u_K} ] \in \mathbb{R}^{D\times K}$. 
  Compute the approximated shape $\mathbf{f}_{\text{new}}\approx\widehat{\mathbf{f}}=\overline{\mathbf{x}} + \mathbf{U} \mathbf{\alpha}$.  
	Visualise your result by plotting the true vs. estimated shape. Compute the error as $||\widehat{\mathbf{f}}-\mathbf{f}||_2^2$? </li>
	<li>  How many components do you need to preserve at least 90% of the variance of the data? </li> 
</ol>
  
Hint for visualisation: The file triangles.mat contains connective information for the face data points. It contains an array with 184 rows, where each row contains the point indices (starting at 1) of points which should be connected by a triangle. 

Hint: How to solve this equation system, revisist the regression tasks. You will find that you can use the psudei inverse.

$$\mathbf{f}_\text{new}-\overline{\mathbf{x}} = \mathbf{U} \mathbf{\alpha}$$

$$\mathbf{U}^T (\mathbf{f}_\text{new}-\overline{\mathbf{x}}) = \mathbf{U}^T\mathbf{U} \mathbf{\alpha}$$

$$(\mathbf{U}^T\mathbf{U})^{-1}\mathbf{U}^T (\mathbf{f}_\text{new}-\overline{\mathbf{x}}) = \mathbf{\alpha}$$
