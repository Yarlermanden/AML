{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Train a GAN yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How many interactions (and time) does it take for you to consider the samples realistic?\n",
    "    - We need at least 1000 steps for the majority of the samples to begin to look like numbers. \n",
    "        - Though some are still way off. At this point the discriminator guesses correctly about 65% of the time (frechlet distance)\n",
    "    - In the end after 5000 steps, most look quite realistic, but some are still quite off. Though it is worth noting that MNIST do have some really weird numbers as well\n",
    "        - At this point the Frechlet distance is about 55, meaning the discriminator still guesses correctly more than half of the time.\n",
    "- How can you judge the quality other than manual visual inspection?\n",
    "    - The Frechlet distance lets us determine how well the generator does compared to the discriminator\n",
    "    - The Inception score is a metric used to determine how realistic the images generated is. They use features extracted from the images to compute conditional and marginal distributions which is used for KL divergence. \n",
    "        - $IS = \\mathbf{exp}(E_x KL(p(y|x)||p(y)))$\n",
    "        - It measures both quality and diversity of the images.\n",
    "- What happens if you alter the hyperparameters in the last two cells and run it again? Select one at a time, e.g. train_batch_size, noise:dimensions, generator_lr, discriminator_lr, steps_per_eval, max_train_steps, batches_for_eval_metrics\n",
    "    - First run took 109min on CPU\n",
    "    - Get it to work with GPU and just make some nested loops of tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Applying StyleGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Use the StyleGAN to generate new samples and interpolate between them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is simply just running the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Select and prepare examples for your discussion in class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is simply just running the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c. Describe what you observe with respect to the results:\n",
    "- Do all samples look realistic?\n",
    "    - Surprisingly realistic, yes!\n",
    "- Do you observe artefacts?\n",
    "    - There might be something about the light\n",
    "    - Furthermore the shoulders/hair seem to be worse than the faces themselves\n",
    "- Observe which parts and features in the faces change while interpolating. Are the interpolated images what you expect?\n",
    "    - Pretty much yes. When interpolating an image of a woman and a man, the inbetween seems like something in between and so on...\n",
    "- In the PCA example: describe the principal components generated from the latent space. Which properties in the face change for each of them?\n",
    "    - Most of them change angle\n",
    "    - Some change smile, age, gender, color\n",
    "- Do you get different results if you run it again?\n",
    "    - No, it doesn't seem so\n",
    "    - It actually seem so!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### d. What happens if you extrapolate instead of interpolating? Can you create an image which does not look like a real face?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, definitely!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Feature visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Refresh yourself on the https://distill.pub/2017/feature-visualization/ blog post that you might have studied before; read through"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribution: which part of an example activates which part of the network\n",
    "\n",
    "Feature visualization: what a network are looking for in examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. After choosing different input images on the top of the blog post, inspect the detectors, e.g. ears and noses for Labrador retriever and tiger cat and flowers for vase and lemon. What could these numbers next to the detector mean relative to each other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers shows how sure the model is of it being dog/cat depending on that feature.\n",
    "\n",
    "e.g. For the labrador ear feature, the model thinks it quite likely a dog and even unlikely it's a cat. Therefore the net evidence is even higher than just the evidence for labrador. By holding over the feature, we also see that it almost only lights up at the dog ear.\n",
    "\n",
    "This means this exact features helps quite a lot to distinguish the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c. Look into what different features the network sees in the four different layers (thus explore the \"semantic dictionary\" that emerged for the image). How are the features different for the layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The abstraction level changes quite a bit:\n",
    "- Edges: detecting simple edges from first 2d convolution\n",
    "- Textures: circles, triangles, squares...\n",
    "- Patterns: bigger picture of the textures\n",
    "- Parts: Almost objects like a nose or something\n",
    "- Objects: Almost objects, but still really weird\n",
    "\n",
    "\n",
    "In the section with the \"What does the Network see\":\n",
    "We see what the network understand of the input image as a whole at this layer.\n",
    "- All of them are quite abstract\n",
    "- in the first layers, we see how it sees almost an abstract version of the image\n",
    "- In the last layer, it almost just sees whether each part is dog, cat or something else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### d. Inspect the spatial attribution (with saliency maps) on the different layers. Focus on how the activation of the learned features or \"visual dictionary\" emerged. Simple question first: why are attribution spaces different? What attribution did you find between the different layers (w.r.t. your input image)? How are the spatial positions different, which the different layers look at?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure\n",
    "\n",
    "We see which neurons in which layers that fire depending on what output we choose. Like why is it the most dog.\n",
    "\n",
    "We can see how the first layer almost outlines the entire dog for \"Labrador Retriever\" - but really lightly.\n",
    "When look at the last layers, it highlights the abstract dogs really well, which is exactly the point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### e. In the final step, the blog post explores how factorising all the feature channels can provide meaningful insight into the decision process of the network. Have the identified neuron groups learned and localised features that are similar to what your brain would spot as important to detect the animals or objects in the image? Now think about data that the network was trained with: can you find features that the network is focussing on that are counter-intuitive and potentially too specific to the image or dataset?\n",
    "Hint: the last section The Space of Interpretability Interfaces provides a summary and discussion about the techniques that are illustrated in the blog post. Write down and bring to class questions that remain open here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('AML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20afb3d869327389044feed7fefb9b458eb6296b59bc368887fba7799b57fd31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
