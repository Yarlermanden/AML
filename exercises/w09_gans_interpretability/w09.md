
# Exercise week 9

This is your 8th mandatory exercise which must be completed until next week, 08.11.2022, 14:00. By that time you must fill out the checklist on the learnIT page to indicate which tasks you completed and volunteer to present.


## Task 1: Train a GAN yourself
Training a GAN is challenging due to the two competing networks. 
Via this [colab notebook](https://colab.research.google.com/github/tensorflow/gan/blob/master/tensorflow_gan/examples/colab_notebooks/tfgan_tutorial.ipynb?utm_source=ss-gan&utm_campaign=colab-external&utm_medium=referral&utm_content=tfgan-intro#scrollTo=AH6gcvcwHvSn) you can train a GAN on MNIST. 

Train at least one GAN yourself.   
 - How many interactions (and time) does it take for you to consider the samples realistic?
 - How can you judge the quality other than manual visual inspection?
 - What happens if you alter the hyperparameters in the last two cells and run it again? Select one at a time, e.g. train_batch_size, noise:dimensions, generator_lr, discriminator_lr, steps_per_eval, max_train_steps, batches_for_eval_metrics


## Task 2: Applying StyleGAN
This task is about exploring StyleGAN, and creating new images. For this, we provide the python notebook w09_StyleGAN.ipynb, which conveniently loads a pre-existing model. 

<ol type ="a">
  <li>Use the StyleGAN to generate new samples and interpolate between them.</li>
  <li>Select and prepare examples for your discussion in class.</li> 
  <li>Describe what you observe with respect to the results:

- Do all samples look realistic? 
- Do you observe artefacts? 
- Observe which parts and features in the faces change while interpolating. Are the interpolated images what you expect?
- In the PCA example: describe the principal components generated from the latent space. Which properties in the face change for each of them? 
- Do you get different results if you run it again?
</li>
 
  <li>What happens if you extrapolate instead of interpolating? Can you create an image which does not look like a real face?</li>
</ol>


## Task 3: Feature visualisation
In this task, we go through the excellent blog post [The Building Blocks of Interpretability](https://distill.pub/2018/building-blocks/) by Chris Olah to understand what a network has learned about an image. In particular, we look **conceptually** into the features that the network learned for representing the aspects of images and how filtered parts of the input are attributed to an output class. We look into one implementation for these two steps in task 9.4.

<ol type ="a">
 <li>Refresh yourself on the https://distill.pub/2017/feature-visualization/ blog post that you might have studied before; Read through </li>
 <li>After choosing different input images on the top of the blog post, inspect the detectors, e.g. ears and noses for <i>Labrador retriever and tiger cat</i> and flowers for <i>vase and lemon</i>. What could these numbers next to the detector mean relative to each other?</li>
 <li>Look into what different features the network sees in the four different layers (thus explore the "semantic dictionary" that emerged for the image). How are the features different for the layers?</li>
 <li>Inspect the <i>spatial attribution (with saliency maps)</i> on the different layers. Focus on how the activation of the learned features or "visual dictionary" emerged. Simple question first: why are the attribution spaces different? What attribution did you find between the different layers (w.r.t. your input image)? How are the spatial positions different, which the different layers look at?</li>
 <li>In the final step, the blog post explores how factorising all the feature channels can provide meaningful insight into the decision process of the network. Have the identified neuron groups learned and localised features that are similar to what your brain would spot as important to detect the animals or objects in the image? Now think about data that the network was trained with (we'll read into ImageNet in task 9.4): Can you find features that the network is focussing on that are counter-intuitive and potentially too specific to the image or dataset?</li>

  <i>Hint</i> the last section <i>The Space of Interpretability Interfaces</i> provides a summary and discussion about the techniques that are illustrated in the blog post. Write down and bring to class questions that remain open here.
</ol>


## Task 4: Understanding Feature Visualisation and Saliency Maps
For this task, we look into an **implementation** of using gradient information to visualise how the network has learned to represent features and to attribute them for an image w.r.t. an output class. The key aim is to look more closely into visualising patterns (features!) that some neurons represent and into the importance, or salience, of specific input patches for the output, this time with a prepared implementation via https://colab.research.google.com/drive/17_DTrB13AemdSjStsnCqJU8TegeOJTq1 (alternatively: w09_visualisation_saliencymaps_pytorch.ipynb on your local machine).

<ol type ="a">
 <li>Read up in more detail on the ImageNet dataset series: https://paperswithcode.com/dataset/imagenet ; Read through the notebook and followup the two explanation blocks (<i>Attribution with Salience Maps</i> and <i>Feature Visualisation</i>).</li>
 <li>Explore the saliency maps on other (i.e. lower) layers. Are other aspects of the images more salient for these layers? Can you come up with a hypothesis why?</li>
 <li>Try different images of different categories (e.g. get some images of the ImageNet dataset or just arbitrary images that match one of the trained classes). What input parts are most salient?</li>
 <li>Explore and visualise what patterns maximise the output for <b>other</b> classes that you find interesting (first for the church images, then for your own from task 9.4.c. The <i>imagenet_classes</i> dictionary can tell you the id for other classes. Did the network identify salient patterns in the images that, in fact, do resemble aspects of your chosen class?</li>
 <li>It has been shown that by adding particular small noise to an image, one can generate *adversarial* images that trick a neural network into confidently classifying them into a wrong category. Try if you can modify the existing code to achieve this!
    <i>Hint:</i> We no longer need to constrain the noise to be spatially smooth, but we want it to be small in general so that the perturbation to the original images is not perceivable by a human. The input to the network should be the perturbed image, not the perturbation itself.</li>
</ol>


## Optional bonus task 5: DCGAN on celebA
If you want to go further on GANs, explore this tutorial on [DCGAN](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html) on the CelebA database, which contains images of celebrities. 
Please note that you need to download data for this. 


## Optional expert task 6: Use GANs to create yourself as art

Take on a kaggle challenge to generate your profile photo (or the photo of your cat) in Monet style!
<ol type ="a">
 <li>Via https://www.kaggle.com/competitions/gan-getting-started/overview get an overview of kaggle and the task.</li>
 <li>Explore the CycleGAN by Amy Jang's tutorial via https://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial/notebook (the notebook is not directly compatible with colab, but if you want, you can import and make the necessary changes to install the kaggle libraries)</li>
</ol>
