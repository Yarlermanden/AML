{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Determining Hidden Markov Model (HMM)\n",
    "\n",
    "##### a. Write down the transition matrix $\\mathbf{A}$ and the emission matrix $\\mathbf{B}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State Transition matrix:\n",
    "$$\n",
    "\\mathbf{A} = \n",
    "\\begin{bmatrix}\n",
    "0.95 & 0.05 \\\\\n",
    "0.1 & 0.9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Emission matrix:\n",
    "$$\n",
    "\\mathbf{B} = \n",
    "\\begin{bmatrix}\n",
    "1/6 & 1/6 & 1/6 & 1/6 & 1/6 & 1/6 \\\\\n",
    "1/10 & 1/10 & 1/10 & 1/10 & 1/10 & 1/2\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. For the observation sequence $\\mathbf{O}=(1,6,6)$, compute the probabilities of the hidden sequences (b1) $\\mathbf{Z}=(F,L,L)$, and (b2) $\\mathbf{Z}=(F,F,F)$, i.e. $P(\\mathbf{X}, \\mathbf{Z}|\\mathbf{\\theta})$. Which is higher?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the decoder, which is probability of hidden states given model and observations.\n",
    "\n",
    "$\\underline{b1 - Z(F,L,L)}$: \n",
    "\n",
    "Probability of the hidden sequence (F,L,L): \n",
    "Start in F, transition to L, stay in L\n",
    "\n",
    "$P(\\mathbf{Z|\\theta}$) = 0.5 * 0.05 * 0.9 = 0.0225\n",
    "\n",
    "Probability of observation sequence (1,6,6) given (F,L,L) is:\n",
    "\n",
    "$P(\\mathbf{O}|\\mathbf{Z}, \\theta)$ = 1/6 * 1/2 * 1/2 = 0.041667\n",
    "\n",
    "$P(\\mathbf{\\mathbf{O},\\mathbf{Z}|\\theta})$ = 0.0225 * 0.041667 = 0.0009375\n",
    "\n",
    "Slides:\n",
    "\n",
    "$P(\\mathbf{O}|\\mathbf{Z}, \\mathbf{\\theta})$ = 0.5 * 1/6 * 0.05 * 1/2 * 0.9 * 1/2 = 0.0009375\n",
    "\n",
    "\n",
    "\n",
    "$\\underline{b2 - Z(F,F,F)}$:\n",
    "\n",
    "Probability of the hidden sequence (F,F,F):\n",
    "Start in F, stay in F, stay in F\n",
    "\n",
    "$P(\\mathbf{Z|\\theta}$) = 0.5 * 0.95 * 0.95 = 0.45125\n",
    "\n",
    "Probability of observation sequence (1,6,6) given (F,F,F):\n",
    "\n",
    "$P(\\mathbf{O}|\\mathbf{Z}, \\theta)$ = 1/6 * 1/6 * 1/6 = 0.0046296\n",
    "\n",
    "$P(\\mathbf{\\mathbf{O},\\mathbf{Z}|\\theta})$ = 0.45125 * 0.0046296 = 0.002089 \n",
    "\n",
    "\n",
    "b2 is the most likely hidden state of the 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c. How can the probability of a sequence given a model $P(\\mathbf{X} | \\mathbf{\\theta})$ be computed? Give the equation in detail, i.e. how all needed $\\mathbf{Z}$ look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the equation for evaluting the model $P(\\mathbf{X}|\\mathbf{\\theta})$.\n",
    "For this we use forward algorithm to recursively calculate partial-sequence probabilities.\n",
    "\n",
    "Hence we need to compute the sum of the partial-sequence probabilities that can lead to this sequence. i.e. find all hidden statees that can lead to this sequence and their corresponding probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the example above (1,6,6), we need to find all hidden states that can lead to this:\n",
    "\n",
    "(F,F,F), (F,F,L), (F,L,F), (F,L,L), (L,L,L), (L,L,F), (L,F,L), (L,F,F)\n",
    "\n",
    "and for each of these, we need to compute the probability of this hidden state to occur times the probability of the hidden state resulting in the sequence observed:\n",
    "\n",
    "With (F,F,F) as example:\n",
    "\n",
    "$P(\\mathbf{X}|(F,F,F))$ 0.5 * 0.95 * 0.95 for the state times 1/6 * 1/6 * 1/6 for the sequence observed = 0.002089\n",
    "\n",
    "This should be summed with all the other combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(\\mathbf{x}|\\mathbf{\\theta})$ = $\\sum_{i=1}^8(P(x,Z_i|\\theta))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### d. Implement a function to compute $P(\\mathbf{X}, \\mathbf{Z} | \\mathbf{\\theta})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(\\mathbf{X}, \\mathbf{Z}|\\mathbf{\\theta}) = P(\\mathbf{X}|\\mathbf{\\theta}) * P(\\mathbf{Z}|\\mathbf{\\theta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement function for forward algorithm\n",
    "\n",
    "def score_sequences(sequences,initial_probs,transition_probs,emission_probs,obs):\n",
    "    \n",
    "    best_score = -1\n",
    "    best_sequence = None\n",
    "    \n",
    "    sequence_scores = []\n",
    "    for seq in sequences:\n",
    "        total_score = 1\n",
    "        total_score_breakdown = []\n",
    "        first = True\n",
    "        for i in range(len(seq)):\n",
    "            state_score = 1\n",
    "            # compute transitition probability score\n",
    "            if first == True:\n",
    "                state_score *= initial_probs[seq[i]]\n",
    "                # reset first flag\n",
    "                first = False\n",
    "            else:  \n",
    "                state_score *= transition_probs[seq[i] + \"|\" + seq[i-1]]\n",
    "            # add to emission probability score\n",
    "            state_score *= emission_probs[obs[i] + \"|\" + seq[i]]\n",
    "            # update the total score\n",
    "            #print state_score\n",
    "            total_score_breakdown.append(state_score)\n",
    "            total_score *= state_score\n",
    "            \n",
    "        sequence_scores.append(total_score)\n",
    "        \n",
    "    return sequence_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### e. Implement the Forward-Algorithm. Compute 6.2(c) using Python code, i.e. define a function, which:\n",
    "- receives as input an observation sequence $\\mathbf{X}$, and\n",
    "- provides the probability of on the observation sequence $P(\\mathbf{X} | \\mathbf{theta})$ as output.\n",
    "Hint: consider how do you get all needed combinations of th hidden state sequences? Then start with a fixed observation length of your choice, e.g. 3, then expand to arbitrary lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Distributions\n",
      "{'F': 0.5, 'L': 0.5}\n",
      "\n",
      "Transition Probabilities\n",
      "+--------+------+-----+\n",
      "| rows   |    F |   L |\n",
      "|--------+------+-----|\n",
      "| F      | 0.95 | 0.1 |\n",
      "| L      | 0.05 | 0.9 |\n",
      "+--------+------+-----+\n",
      "\n",
      "Emission Probabilities\n",
      "+--------+----------+-----+\n",
      "|   rows |        F |   L |\n",
      "|--------+----------+-----|\n",
      "|      4 | 0.166667 | 0.1 |\n",
      "|      2 | 0.166667 | 0.1 |\n",
      "|      6 | 0.166667 | 0.5 |\n",
      "|      1 | 0.166667 | 0.1 |\n",
      "|      3 | 0.166667 | 0.1 |\n",
      "|      5 | 0.166667 | 0.1 |\n",
      "+--------+----------+-----+\n",
      "\n",
      "Scores\n",
      "Sequence:['F', 'F', 'F'],Prob:0.0021\n",
      "Sequence:['F', 'F', 'L'],Prob:0.0003\n",
      "Sequence:['F', 'L', 'F'],Prob:0.0000\n",
      "Sequence:['F', 'L', 'L'],Prob:0.0009\n",
      "Sequence:['L', 'F', 'F'],Prob:0.0001\n",
      "Sequence:['L', 'F', 'L'],Prob:0.0000\n",
      "Sequence:['L', 'L', 'F'],Prob:0.0004\n",
      "Sequence:['L', 'L', 'L'],Prob:0.0101\n",
      "\n",
      "Scores with the hidden sequence\n",
      "Hidden Sequence:['F', 'F', 'F'], Yields prob:0.0021\n"
     ]
    }
   ],
   "source": [
    "# given states - what are the possible combinations\n",
    "# total number of combinations is (number of possible states)^(sequence length)\n",
    "def generate_sequence(states,sequence_length):\n",
    "    \n",
    "    all_sequences = []\n",
    "    nodes = []\n",
    "    \n",
    "    depth = sequence_length\n",
    "    \n",
    "    def gen_seq_recur(states,nodes,depth):\n",
    "        if depth == 0:\n",
    "            #print nodes\n",
    "            all_sequences.append(nodes)\n",
    "        else:\n",
    "            for state in states:\n",
    "                temp_nodes = list(nodes)\n",
    "                temp_nodes.append(state)\n",
    "                gen_seq_recur(states,temp_nodes,depth-1)\n",
    "    \n",
    "    gen_seq_recur(states,[],depth)\n",
    "                \n",
    "    return all_sequences\n",
    "\n",
    "\n",
    "\n",
    "# pretty printing our  distributions\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "def pretty_print_probs(distribs):\n",
    "    \n",
    "    rows = set()\n",
    "    cols = set()\n",
    "    for val in distribs.keys():\n",
    "        temp = val.split(\"|\")\n",
    "        rows.add(temp[0])\n",
    "        cols.add(temp[1])\n",
    "        \n",
    "    rows = list(rows)\n",
    "    cols = list(cols)\n",
    "    df = []\n",
    "    for i in range(len(rows)):\n",
    "        temp = []\n",
    "        for j in range(len(cols)):\n",
    "            temp.append(distribs[rows[i]+\"|\"+cols[j]])\n",
    "            \n",
    "        df.append(temp)\n",
    "        \n",
    "    I = pd.Index(rows, name=\"rows\")\n",
    "    C = pd.Index(cols, name=\"cols\")\n",
    "    df = pd.DataFrame(data=df,index=I, columns=C)\n",
    "    \n",
    "    print(tabulate(df, headers='keys', tablefmt='psql'))\n",
    "def initializeSequences(_obs):\n",
    "    # Generate list of sequences\n",
    "    \n",
    "    seqLen = len(_obs)\n",
    "    seqs = generate_sequence(states,seqLen)\n",
    "    # Score sequences\n",
    "    seq_scores = score_sequences(seqs,initial_probs,transition_probs,emission_probs,obs)\n",
    "    \n",
    "    return (seqLen,seqs,seq_scores)\n",
    "\n",
    "# We can use a dictionary to capture our state transitions\n",
    "# set of hidden states\n",
    "states = ['F','L']\n",
    "# set of observations\n",
    "obs = ['1','6','6']\n",
    "# set of hidden layers\n",
    "hidden = ['F','F','F']\n",
    "# initial state probability distribution (our priors)\n",
    "initial_probs = {'F':0.5,'L':0.5}\n",
    "# transition probabilities\n",
    "transition_probs = {'F|F':0.95,'L|F':0.05,'F|L':0.1,'L|L':0.9}\n",
    "# emission probabilities\n",
    "emission_probs = {'1|F':1/6, '2|F':1/6, '3|F':1/6, '4|F':1/6, '5|F':1/6, '6|F':1/6, '1|L':1/10, '2|L':1/10, '3|L':1/10, '4|L':1/10, '5|L':1/10, '6|L':1/2}\n",
    "# Generate list of sequence\n",
    "sequence_length,sequences,sequence_scores = initializeSequences(obs)\n",
    "# print results\n",
    "print(\"Initial Distributions\")\n",
    "print(initial_probs)\n",
    "print(\"\\nTransition Probabilities\")\n",
    "pretty_print_probs(transition_probs)\n",
    "print(\"\\nEmission Probabilities\")\n",
    "pretty_print_probs(emission_probs)\n",
    "print(\"\\nScores\")\n",
    "# Display sequence scores\n",
    "for i in range(len(sequences)):\n",
    "    print(\"Sequence:%10s,Prob:%0.4f\" % (sequences[i],sequence_scores[i]))\n",
    "\n",
    "# Display prob of obs given sequence\n",
    "print(\"\\nScores with the hidden sequence\")\n",
    "for i in range(len(sequences)):\n",
    "    if sequences[i] == hidden:\n",
    "        print(\"Hidden Sequence:%10s, Yields prob:%0.4f\" % (sequences[i],sequence_scores[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Understanding Basic Recurrent Neural Networks (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Retrace the data loading and preprocessing and take your time to clarify:\n",
    "- How and why is the vocabulary dictionary built up?\n",
    "    - It's built up on token level and depends on the frequency of each token\n",
    "    - Each token can either be a word or a character\n",
    "    - The vocabulary consists of vectors of 0 except for the index at the word: for one-hot encoding\n",
    "    - self.token_freqs -> tokens and counts of tokens in most frequent order\n",
    "    - self.uniq_tokens -> unique tokens with a frequence above som threshold\n",
    "    - self.idx_to_tokens -> way of getting each token from index\n",
    "    - self.tokens_to_idx -> way of getting index of each token\n",
    "- What is the role of \"num_steps\"?\n",
    "    - The length of sequences\n",
    "    - We need this to standardize the inputs.\n",
    "        - if not the same length, we use padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Retrace the model (first the implementation from scratch, then the RNN sub-module of the framework) and familiarize yourself with the data structure of the network tensor: ('num_steps', 'batch_size', 'vocab_size')\n",
    "- Why is it a good idea to fix these parameters in the model building?\n",
    "    - The different inputs need the same model layout to compile and function\n",
    "    - Enables in determining the other hyperparameters: weight matrix and bias vector.\n",
    "- What is the effect of a small versus a large vocab_size?\n",
    "    - Large vocab_size requires a lot more training data but could probably result in more fine tuned language in the end. For this data though, it quickly gets too large, so it's best to keep small for actually getting words.\n",
    "    - Smaller vocab gives smaller computations. Larger gives much more computations. \n",
    "    - The dimensionality of the space gets higher with more vocabs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c. Retrace the training steps and focus on the following questions:\n",
    "- How and why are hidden states initialized during training batches (and how is this different to non-Recurrent NN training)?\n",
    "    - state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "    - Hidden states are initialized during training batches as they depent on the previous outputs predicted. We therefore need each training batch to have a clean start if we use random sequence.\n",
    "    - It's not relevant for non-Recurrent NN training as the current prediction isn't dependent on the previous predictions. \n",
    "- Specifically: what would be the effect of initializing vs not initializing hidden states at the begining of each first time step of a sequence?\n",
    "    - It simulates no dependency on the previous sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### d. Initialize the hidden states (init_rnn_states) instead of with zeros with other values (e.g. random values between 0 and 1).\n",
    "- What is the effect?\n",
    "    - Seems to get better values -> because they are closer than all at zero?\n",
    "    - Starting it with random numbers forces the model to think this depends on something it has learned in the past.\n",
    "- How does the training behave when we use no gradient clipping?\n",
    "    - Didn't test but would probably get exploding gradients. The weights get huge.\n",
    "- What is the effect on the training when using other values for num_steps: e.g. 15, 60, and 120?\n",
    "    - No real difference. Same performance speed and outcomes\n",
    "    - Small number of steps prevents the model from retaining a large memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus: Perplexity is a way of counting the likelihood by counting. This is to prevent just looking at the probability, which would almost always be so small the computer would translate to 0, due to there being so many different combinations of words that can make up a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Understanding Gating in RNNs\n",
    "Retrace the implementation of the model and:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Modify the example to run with the SimpleRNN instead of the GRU and note down your result as a baseline. How did the network perform (quality)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Now run the model using GRU: what are the major effects in training (speed and convergence) and results (quality)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c. Modify the example to run with LSTM cells instead:\n",
    "- What is the effect?\n",
    "- Is the performance different thatn compared to using GRU?\n",
    "    - Should be quicker trained\n",
    "    - Better trained as it can retain information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### d. Compare the parameters of the needed number for decent performance between GRU, LSTM and the baseline (the baseline/SimpleRNN might not converge)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Understanding Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. So, first of all, explore learned word embeddings with Word2Vec\n",
    "- What is the difference in data and effect of Word2Vec All, Word2Vec 10k?\n",
    "    - One of them as a lot more vectors (71k) compared to only 10k\n",
    "    - Due to it mapping all words to vectors\n",
    "    - word2Vec all - suggest/maps a lot more rare words 0 which can be both for better and worse\n",
    "        - probably more nuanced suggestions, but can also be more confusing or not what we expect\n",
    "- What are your most 1) intuitive and 2) counter-intuitive observations regarding the vector spaces?\n",
    "    - Kg -> both maps a lot of weight measurring units\n",
    "    - irrelevent (spelling mistake?) -> mapped to other useless stuff\n",
    "        - Doesn't even exist in Word2Vec 10k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Next, reproduce how building embeddings from skip-grams work (again, focus on the questions below, otherwise you might drown in code and details)\n",
    "- Refresh yourself and understand: how is the tokenisation realized?\n",
    "- Try different embedding sizes. How do similar tokens compare? Test own token examples as well!\n",
    "- Figure out and test different window sizes for the skip-grams. What is the impacto n quality (e.g. similarity of tokens) vs. effor (training speed and convergence)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('AML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20afb3d869327389044feed7fefb9b458eb6296b59bc368887fba7799b57fd31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
