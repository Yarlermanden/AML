
# Exercise week 3

Welcome to the course Advanced Machine Learning! This is your third mandatory exercise which must be completed until next week, 20.09.2022, 14:00. By that time you must fill out the checklist on the learnit page to indicate which tasks you completed and volunteer to present. 

## Task 1: SVM - Theory and Practice 
NOTE: You are free to choose if you want to solve the with pen and paper only, or via some programming help, as in using wolfram alpha and/or python. If you choose to do the latter, you are still required to implement the equations. 
The XOR problem consists of four points $\mathbf{x}_n$, $n=1,\ldots,4$ from two classes, which are not linearly separable, as follows:
 
<ol>
 <li> class 1: $\mathbf{x}_1=\begin{pmatrix} 1\\ 1 \end{pmatrix}$, $\mathbf{x}_2=\begin{pmatrix} -1\\ -1 \end{pmatrix}$, with labels $t_1=+1$, $t_2=+1$</li>
 <li> class 2: $\mathbf{x}_3=\begin{pmatrix} -1\\ 1 \end{pmatrix}$, $\mathbf{x}_4=\begin{pmatrix} 1\\ -1 \end{pmatrix}$, with labels $t_3=-1$, $t_4=-1$.</li>
</ol>

The goal of this exercise is to compute the discriminant
$$g(\mathbf{x}) = \mathbf{w}^T \mathbf{\phi}(\mathbf{x}), $$
which enables a linear classification in a higher dimension using the basis function $\mathbf{\phi}$.

<ol type ="a">
 <li> Draw the points and plot them such that the two different classes can be distinguished (either by color and or marker). </li>
 <li> Since the points are not linearly separable in 2D, they will be transferred to a higher dimension in an attempt to make them linearly separable in a higher dimension. Use the following basis function to transfer each of the four 2D points to 6D: $\mathbf{\phi} :\mathbb{R}^2 \rightarrow \mathbb{R}^6$ 
	$$ \mathbf{\phi}(\mathbf{x}) = \mathbf{\phi} (x_1,x_2) = \left( 1,~ \sqrt{2}~ x_1,~ \sqrt{2}~ x_2,~ \sqrt{2} ~x_1 x_2 ,~ x_1^2 ,~ x_2^2\right)^T, $$
	 i.e. calculate $\mathbf{z}_n=\mathbf{\phi}(\mathbf{x}_n)$, $n=1,\ldots,4=N$. 
	</li>
 <li> Use the known values to complete Eq. (7.10) from Bishop [PR] 
	$$\tilde{L}(\mathbf{a})=\tilde{L}\left(a_1,a_2,a_3,a_4 \right) =\sum\limits_{n=1}^4 a_n -\frac{1}{2} \sum\limits_{m=1}^4 \sum\limits_{n=1}^4 a_n a_m t_n t_m \mathbf{\phi}(\mathbf{x}_n)^T\mathbf{\phi}(\mathbf{x}_m),$$
	 where $k(\mathbf{x}_n,\mathbf{x}_m)=\mathbf{\phi}(\mathbf{x}_n)^T\mathbf{\phi}(\mathbf{x}_m)$. Note: The values $a_n$ are unknown at this point. A solution for this specific data set will be derived by you in task e. 
	</li> 
 <li> Compute the derivative of $\tilde{L}(\mathbf{a} )$ with respect to $a_k$, i.e. the four components of the gradient 
	$$\nabla \tilde{L}(\mathbf{a}) =  \begin{pmatrix} \frac{\partial}{\partial a_1} \tilde{L}\left(a_1,a_2,a_3,a_4 \right) \\				\frac{\partial}{\partial a_2} \tilde{L}\left(a_1,a_2,a_3,a_4 \right)\\				\frac{\partial}{\partial a_3} \tilde{L}\left(a_1,a_2,a_3,a_4 \right)\\				\frac{\partial}{\partial a_4} \tilde{L}\left(a_1,a_2,a_3,a_4 \right) \end{pmatrix} = \ldots  $$
	</li> 
 <li> Derive the equation system from $\nabla\tilde{L}(\mathbf{a})=0$ and solve for $\mathbf{a}=\left(a_1,a_2,a_3,a_4 \right)^T$. 
	Hint: One example to rewrite a 2D gradient of a function into a non-homogeneous equation system: $$ \nabla f(\mathbf{x})= \begin{pmatrix} x_1+3x_2 + b_1\\2x_1-x_2+b_2\end{pmatrix} = \begin{pmatrix} 1 &3\\2& -1\end{pmatrix}\begin{pmatrix} x_1\\x_2\end{pmatrix}+ \begin{pmatrix} b_1\\ b_2 \end{pmatrix}= \mathbf{A}\mathbf{x}+\mathbf{b} \stackrel{!}{=} \mathbf{0}$$
	</li> 
 <li> Which of the four training points are support vectors? How do the values of $a_n$ answer this question? </li>
 <li> Now that all four values of $\mathbf{a}$ have been computed, employ Eq.(7.29) to compute $\mathbf{w}$:
	$$ \mathbf{w} = \sum_{n=1}^4 a_n  t_n \mathbf{z}_n = \sum_{n=1}^4 a_n  t_n \mathbf{\phi}(\mathbf{x}_n) $$
	Please note: $\mathbf{w},\mathbf{z}_n\in\mathbb{R}^6$ and $t_n\in\mathbb{R} $.  </li>	
 <li> Give the discriminant function $g$ based on the original input space 
	 $$ g(\mathbf{x}) = g(x_1,x_2)= \mathbf{w}^T \mathbf{\phi}(\mathbf{x})= \ldots $$
	</li>		
 <li> Apply the discriminant function and compute the resulting values for the training input samples $g(\mathbf{x}_i)$, $i=1,\ldots, 4$. How are they classified? Are they correctly classified? </li>
</ol>


## Task 2: Programming Exercise - kernel SVM
The goal is to implement and evaluate different kernels for SVMs for one dataset. 
For this programming exercise the notebook **w03_svm_sklearn.ipynb** (or optionally **w03_svm_tensorflow.ibynb** for students experienced in tensorflow) is provided, which should be used and adapted. 

<ol type ="a">
 <li> Implement:
	<ol type ="a">
   <li> a linear kernel </li>
   <li> a radial basis function kernel</li> 
   <li> a polynomial kernel</li> 
    </ol>
	</li> 
	<li> Which of these performs best on the data, in terms of speed and quality? (Do not forget to set the random seed to receive reproducible results.)</li> 
	<li> Test different values of $c$ and $d$ for the polynomial kernel. Which of them work best?</li> 
	<li> Test different values of $\gamma$ for the RBF kernel. Which of them works best?</li> 
	<li> Change the part of the code which generates the data such that it becomes linearly separable.</li> 
	<li> Re-evaluate the three kernels. Do you get the same result?</li> 
</ol>


## Task 3: Programming an MLP

In the following the task we explore characteristics of training a Multilayer Perceptron (MLP). At first, we use the MLP implementation of the sklearn module. You can use the prepared notebook <b>w03_regression_mlp_iris_sklearn.ipynb</b> for this.

<ol type ="a">
 <li>Empirically get familiar with the MLP, so test different layer sizes on the IRIS data. What configuration results in the best accuracy?</li>
 <li>Test different learning rates, momentum, and number of training iterations. How do these basic hyperparameters affect the convergence to a high accuracy? What values are best?</li>
</ol>

## Task 4: Programming tensorflow vs. pytorch
Now, secondly, we look into the implementation of an MLP in either of the major deep learning frameworks: Tensorflow or PyTorch. You can use the prepared notebooks in <b>w03_mlp_mnist_pytorch.ipynb</b> and <b>w03_mlp_mnist_tensorflow.ipynb</b> for the start. 

<ol type ="a">
 <li>Second, get familiar with either PyTorch or Tensorflow. Test different optimizers and loss functions on the IRIS and MNIST data. What is the impact of hyperparameters for both performance AND speed?</li>
 <li>Test different activation functions. How are loss function and activation function dependent?</li>
</ol>
