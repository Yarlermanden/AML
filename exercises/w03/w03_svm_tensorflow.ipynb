{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 3.2: kernel SVM with tensorflow (optional alternative)\n",
    "\n",
    "Advanced Machine Learning for KCS\n",
    "\n",
    "by Stella Grasshof, Stefan Heinrich, Laura Weihl\n",
    "with material by Kevin Murphy\n",
    "\n",
    "All info and static material: https://learnit.itu.dk/course/view.php?id=3021295\n",
    "\n",
    "-------------------------------------------------------------------------------"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*The goal is to implement and evaluate different kernels for SVMs for one dataset.*\n",
    "*We start by importing the necessary libraries.*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import matplotlib as mpl\r\n",
    "# if you're using tensorflow v1, then uncomment the following one line\r\n",
    "#import tensorflow as tf\r\n",
    "# if you're using tensorflow v2, then uncomment the following two lines\r\n",
    "import tensorflow.compat.v1 as tf\r\n",
    "tf.disable_v2_behavior()\r\n",
    "\r\n",
    "from sklearn import datasets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Creating dataset\n",
    "\n",
    "We generate our random dataset; this will be 2D data that is not linearly separable. In fact, the data will follow concentric rings."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "N = 200 # number of samples\r\n",
    "c = 0.5 # scale factor between inner and outer circles\r\n",
    "noise = 0.1 # noise parameter\r\n",
    "\r\n",
    "# generate data\r\n",
    "x_vals, t_vals = datasets.make_circles(n_samples=N, factor=c, noise=noise)\r\n",
    "# if a value in y_vals is 1, we leave it at one, but if it is 0, we set it to -1\r\n",
    "t_vals = np.where(t_vals, 1, -1)\r\n",
    "\r\n",
    "class1_idxs = np.flatnonzero(t_vals == 1)\r\n",
    "class1_x = x_vals[class1_idxs]\r\n",
    "class1_t = t_vals[class1_idxs]\r\n",
    "class2_idxs = np.flatnonzero(t_vals == -1)\r\n",
    "class2_x = x_vals[class2_idxs]\r\n",
    "class2_t = t_vals[class2_idxs]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can take a quick look at our data to get a sense of what we're trying to predict."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.scatter(class1_x[:, 0], class1_x[:, 1],\r\n",
    "            label = \"Class 1 (+1)\",\r\n",
    "            color = \"none\",\r\n",
    "            edgecolor = \"red\"\r\n",
    "           )\r\n",
    "plt.scatter(class2_x[:, 0], class2_x[:, 1],\r\n",
    "            label = \"Class 2 (-1)\",\r\n",
    "            color = \"none\",\r\n",
    "            edgecolor = \"blue\"\r\n",
    "           )\r\n",
    "plt.legend(loc=\"upper left\", framealpha=0.25)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start a computational graph session."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sess = tf.Session()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We declare the batch size (large for SVMs - in this case we set it equal to the number of samples), create the placeholders, and declare the $\\pmb{\\alpha}$-variable for the SVM model."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "batch_size = N\r\n",
    "\r\n",
    "# initialize placeholders\r\n",
    "x = tf.placeholder(shape=[None, 2], dtype=tf.float32)\r\n",
    "t = tf.placeholder(shape=[None, 1], dtype=tf.float32)\r\n",
    "pred_grid = tf.placeholder(shape=[None, 2], dtype=tf.float32)\r\n",
    "\r\n",
    "# create variables for svm\r\n",
    "alpha = tf.Variable(tf.random_normal(shape=[1, batch_size]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task:\n",
    "### (a)\n",
    "*Implement:*\n",
    "* *a linear kernel:* $K(\\mathbf{x}_1, \\mathbf{x}_2) = \\mathbf{x}_1^\\intercal \\mathbf{x}_2$\n",
    "* *a Gaussian or radial basis function (RBF) kernel:* $K(\\mathbf{x}_1, \\mathbf{x}_2) = e^{(-\\gamma \\|\\mathbf{x}_1 - \\mathbf{x}_2 \\|^2)}$\n",
    "* *a polynomial kernel:* $K(\\mathbf{x}_1, \\mathbf{x}_2) = (\\mathbf{x}_1^\\intercal \\mathbf{x}_2 + c)^d$\n",
    "\n",
    "#### Here can can write your solution:"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# linear kernel\r\n",
    "# *CODE FOR LINEAR KERNEL*\r\n",
    "#kernel = *kernel output value*\r\n",
    "kernel = tf.matmul(x, tf.transpose(x))\r\n",
    "\r\n",
    "# Gaussian (RBF) kernel\r\n",
    "# *CODE FOR GAUSSIAN (RBF) KERNEL*\r\n",
    "#kernel = *kernel output value*\r\n",
    "\r\n",
    "# polynomial kernel\r\n",
    "# *CODE FOR POLYNOMIAL KERNEL*\r\n",
    "#kernel = *kernel output value*"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Just like we created the kernel for the training points, we need to create the kernel for the test/prediction points.\n",
    "\n",
    "Again, comment/uncomment the appropriate lines for using the linear, RBF, or polynomial kernels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# prediction kernels\r\n",
    "\r\n",
    "# linear prediction kernel\r\n",
    "# *CODE FOR LINEAR KERNEL*\r\n",
    "#pred_kernel = *kernel output value*\r\n",
    "pred_kernel = tf.matmul(x, tf.transpose(x))\r\n",
    "\r\n",
    "# Gaussian (RBF) prediction kernel\r\n",
    "# *CODE FOR GAUSSIAN (RBF) KERNEL*\r\n",
    "#pred_kernel = *kernel output value*\r\n",
    "\r\n",
    "# polynomial prediction kernel\r\n",
    "# *CODE FOR POLYNOMIAL KERNEL*\r\n",
    "#pred_kernel = *kernel output value*"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we set up the SVM model and create our loss function. This is done using equation (7.10)\n",
    "\n",
    "$$\n",
    "L_d = \\sum_{n} a_n - \\frac{1}{2} \\sum_{n} \\sum_{m} a_n a_m t_n t_m  K(\\mathbf{x}_n, \\mathbf{x}_m),\n",
    "$$\n",
    "\n",
    "however, for TensorFlow we work with arrays, so by defining the matrix $\\mathbf{A} = K(\\mathbf{x}_n, \\mathbf{x}_m) \\pmb{a}^\\intercal \\pmb{a} \\mathbf{t} \\mathbf{t}^\\intercal$, we can sum all its elements to get the double-sum on the right. We can therefore write $L_d$ as\n",
    "\n",
    "$$\n",
    "L_d = \\sum_{n} a_n - \\frac{1}{2} \\sum_{i,j} A_{i,j}\n",
    "$$\n",
    "\n",
    "instead, where $A_{i,j}$ is the value at the $i$th row and $j$th column.\n",
    "\n",
    "*Note that $\\mathbf{A}$ is in fact a matrix since $K(\\mathbf{x}_n, \\mathbf{x}_m)$ gives a scalar, $\\pmb{a}^\\intercal \\pmb{a}$ gives us ($1\\times N \\cdot N\\times 1$) another scalar, and $\\mathbf{t} \\mathbf{t}^\\intercal$ gives us ($N \\times 1 \\cdot 1 \\times N$) an $N \\times N$-matrix.*"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# compute SVM model\r\n",
    "\r\n",
    "alpha_sum = tf.reduce_sum(alpha)\r\n",
    "\r\n",
    "t_matrix = tf.matmul(t, tf.transpose(t))\r\n",
    "alpha_prod = tf.matmul(tf.transpose(alpha), alpha)\r\n",
    "double_sum = tf.reduce_sum(tf.multiply(kernel, tf.multiply(alpha_prod, t_matrix)))\r\n",
    "\r\n",
    "loss = tf.negative(tf.subtract(alpha_sum, double_sum))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to use the kernel to classify points, we create a prediction operation.  This prediction operation will be the sign (positive or negative) of the model outputs.  The accuracy can then be computed if we know the actual target labels."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pred_output = tf.matmul(tf.multiply(tf.transpose(t), alpha), pred_kernel)\r\n",
    "pred = tf.sign(pred_output - tf.reduce_mean(pred_output))\r\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.squeeze(pred), tf.squeeze(t)), tf.float32))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now declare the optimizer and variable initialization operations."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# declare optimizer\r\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.0005)\r\n",
    "train_step = my_opt.minimize(loss)\r\n",
    "\r\n",
    "# initialize variables\r\n",
    "init = tf.global_variables_initializer()\r\n",
    "sess.run(init)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start the training loop for the SVM.  We will randomly choose a batch of points and run the train step.  Then we calculate the loss and accuracy."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# training loop\r\n",
    "temp_losses = []\r\n",
    "batch_accs = []\r\n",
    "np.random.seed(0) # set this for your experiments to compare the different kernels\r\n",
    "for i in range(10000):\r\n",
    "    # generate random indices equal to batch_size \r\n",
    "    batch_idxs = np.random.choice(N, size=batch_size)\r\n",
    "    # get the corresponding input and target points\r\n",
    "    batch_x = x_vals[batch_idxs]\r\n",
    "    batch_t = np.transpose([t_vals[batch_idxs]])\r\n",
    "    # train the model with this batch\r\n",
    "    sess.run(train_step, feed_dict={x: batch_x, t: batch_t})\r\n",
    "    \r\n",
    "    # calculate temporary train loss and accuracy\r\n",
    "    temp_loss = sess.run(loss, feed_dict={x: batch_x, t: batch_t})\r\n",
    "    temp_losses.append(temp_loss)\r\n",
    "    batch_acc = sess.run(accuracy, feed_dict={x: batch_x, t: batch_t, pred_grid: batch_x})\r\n",
    "    batch_accs.append(batch_acc)\r\n",
    "    \r\n",
    "    if (i+1)%1000==0:\r\n",
    "        print(\"Step #{}\".format(i+1))\r\n",
    "        print(\"Loss = \", temp_loss)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To plot a pretty picture of the regions we fit, we create a fine mesh to run through our model and get the predictions."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# find boundaries for contour plot\r\n",
    "abscissa_min, abscissa_max = x_vals[:, 0].min()-1, x_vals[:, 0].max()+1\r\n",
    "ordinate_min, ordinate_max = x_vals[:, 1].min()-1, x_vals[:, 1].max()+1\r\n",
    "\r\n",
    "# generate mesh grid of points\r\n",
    "xx, yy = np.meshgrid(\r\n",
    "    np.linspace(abscissa_min, abscissa_max, 1000),\r\n",
    "    np.linspace(ordinate_min, ordinate_max, 1000)\r\n",
    ")\r\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()]\r\n",
    "\r\n",
    "# find predictions for grid of points\r\n",
    "[grid_preds] = sess.run(pred, feed_dict={\r\n",
    "    x: x_vals,\r\n",
    "    t: np.transpose([t_vals]),\r\n",
    "    pred_grid: grid_points\r\n",
    "})\r\n",
    "grid_preds = grid_preds.reshape(xx.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we make the plot of our points and our decision boundary."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plot our decision boundary\r\n",
    "plt.imshow(\r\n",
    "    grid_preds,\r\n",
    "    extent=[abscissa_min, abscissa_max, ordinate_min, ordinate_max],\r\n",
    "    origin=\"lower\",\r\n",
    "    cmap=\"bwr\",\r\n",
    "    aspect=\"auto\",\r\n",
    "    alpha=0.375\r\n",
    ")\r\n",
    "plt.contour(xx, yy, grid_preds, 1, colors=\"black\", alpha=0.5)\r\n",
    "\r\n",
    "# plot our points\r\n",
    "plt.scatter(class1_x[:, 0], class1_x[:, 1],\r\n",
    "    label = \"Class 1 (+1)\",\r\n",
    "    color = \"none\",\r\n",
    "    edgecolor = \"red\"\r\n",
    ")\r\n",
    "plt.scatter(class2_x[:, 0], class2_x[:, 1],\r\n",
    "    label = \"Class 2 (-1)\",\r\n",
    "    color = \"none\",\r\n",
    "    edgecolor = \"blue\"\r\n",
    ")\r\n",
    "\r\n",
    "# add title and legend\r\n",
    "plt.title(\"Decision boundary of trained SVM\")\r\n",
    "plt.legend(loc=\"upper left\", framealpha=0.25)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also plot the loss over time and batch accuracy."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = plt.figure(figsize=(13,4))\r\n",
    "\r\n",
    "# plot batch accuracy\r\n",
    "ax1 = fig.add_subplot(121)\r\n",
    "ax1.plot(batch_accs, color=\"black\", linewidth=0.1)\r\n",
    "ax1.set_title(\"Accuracy per batch\")\r\n",
    "ax1.set_xlabel(\"Batch\")\r\n",
    "ax1.set_ylabel(\"Accuracy\")\r\n",
    "\r\n",
    "# plot loss over time\r\n",
    "ax2 = fig.add_subplot(122)\r\n",
    "ax2.plot(temp_losses, color=\"black\", linewidth=0.1)\r\n",
    "ax2.set_title(\"Loss per batch\")\r\n",
    "ax2.set_xlabel(\"Batch\")\r\n",
    "ax2.set_ylabel(\"Loss\")\r\n",
    "\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task:\n",
    "### (b)\n",
    "*Which of these performs best on the data, in terms of speed and quality? Do not forget to set the random seed to receive reproducible results.*"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (c)\n",
    "*Test different values of $c$ and $d$ for the polynomial kernel. Which of them work best?*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (d)\n",
    "*Test different values of $\\gamma$ for the RBF kernel. Which of them works best?*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (e)\n",
    "*Change the part of the code which generates the data such that it becomes linearly separable.*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (f)\n",
    "*Re-evaluate the three kernels. Do you get the same result?*"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('.venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "interpreter": {
   "hash": "39711fa3f91d767f7faecc7fbab74d0d9168c9fcaf529a8bc1facbd9166a1928"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}