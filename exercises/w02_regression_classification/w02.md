
# Exercise week 2
Welcome to week 2! Please note that the programming exercises should be implemented by you, not by using a library. The intention is that you learn the transition from equations to code. 
Later in the course you are welcome to use programming frameworks. 

Standard regression model reminder. The standard linear model for regression is defined as follows
$$y_n=f(\mathbf{x}_n)=\mathbf{w}^T\mathbf{x}_n + \epsilon,\quad \epsilon \sim \mathcal{N}(0,\sigma^2),~\mathbf{w}=(w_0,w_1)^T.$$

For the regression tasks, please use the file points.txt which contains 20 samples for a 1D regression problem. It defines the training data: $\mathcal{X}=(x_n,y_n),~n=1,\ldots, N=20$. 
Given this 1D data, and assuming a parametric model, we can approximate the true outputs as follows $y_n\approx \widehat{y}_n=\widehat{w}_0+ \widehat{w}_1 x_n$. 

## Task 1: Programming - Bayesian Regression
Implement a the Bayesian Regression approach for 1D data using the data provided in points.txt.
<ol type ="a">
 <li>Estimate $w_0 , w_1$ by Bayesian estimation as follows. 
	Assuming a Gaussian prior $p(\mathbf{w}) \sim\mathcal{N}(\mathbf{0},(1/\alpha) \mathbf{I})$ yields the posterior $p(\mathbf{w}|\mathcal{X}) \sim\mathcal{N}(\mathbf{\mu}_N, \Sigma_N)$, with
	 $$ \mathbf{\mu}_N= \beta \Sigma_N \mathbf{X}^T\mathbf{y}, $$
	 $$ \Sigma_N = (\alpha\mathbf{I} +\beta \mathbf{X}^T\mathbf{X})^{-1}, $$
	where $\mathbf{I}$ is the identity matrix, and $\mathbf{X}$ is the design matrix (= x values and column of 1). Set $\alpha=2$, $\beta=25$. To receive one estimate $\widehat{\mathbf{w}}=(\widehat{w}_0,~\widehat{w}_1)^T$, get one sample from the posterior $p(\mathbf{w}|\mathcal{X})$. 	
	</br>Hint: w = np.random.multivariate_normal(m_N.ravel(), S_N, num_w_samples).T  </li>
<li>Create $10$ different estimates for lines from (b) by sampling $\widehat{\mathbf{w}}$ to predict $y_n$. From these different lines, estimate the average line. </li>
 <li> Approximate the true datapoints $(x_n,y_n)$ by computing $\widehat{y}_n=\widehat{h}(x_n)=\widehat{w}_0+\widehat{w}_1 x_n$. </li>
 <li> Compute the correlation between true $\mathbf{y}$ and estimated $\widehat{\mathbf{y}}$ predictions.</li>
</ol>

## Task 2: Programming - Nonparametric Regression
Implement 1D nonparametric regression using the kernel smoother, as described in the following using the data provided in points.txt.

<ol type ="a">
 <li> Implement the following function
$$ \widehat{g}(x) = \frac{\sum\limits_{n=1}^{N} \varphi\left( \frac{x-x_n}{h} \right) y_n}{\sum\limits_{n=1}^N \varphi\left( \frac{x-x_n}{h} \right)}, $$ 
	 where $\varphi(x) = \mathcal{N}(0,1)$.  </li>
 <li> Approximate the true datapoints $(x_n,y_n)$ by computing $\widehat{y}_n=\widehat{g}(x_n)$. </li>
 <li> Compute the correlation between true $\mathbf{y}$ and estimated outputs $\widehat{\mathbf{y}}$.</li>
</ol>

Hint: If you struggle with (a), try to use the box function instead of the Gaussian pdf. 
$\varphi(x) = \begin{cases} 1 &,  \text{ if} |x|<1 \\\ 0 &,\text{ else} \end{cases}$


## Task 3: Comparison
<ol type ="a">
 <li> Repetition: estimate $w_0 , w_1$ by the standard least square estimate. </li>
 <li> Approximate the true datapoints $(x_n,y_n)$ by computing $\widehat{y}_n$. </li>
 <li> Compute the correlation between true $\mathbf{y}$ and estimated $\widehat{\mathbf{y}}$ predictions.</li>
 <li> Compare correlation coefficients between the different models. Which is the best based on that metric? </li>
</ol>

## Task 4: Programming Exercise - Classification
The provided iris data consists of a total of 150 samples $\mathbf{x}_n \in \mathbb{R}^{4}$, $n=1,\ldots,150$, of three classes, where each data point has 4 dimensions. 

<ol type ="a">
 <li> Read the data and the labels, and plot the data.  (For reference: Each of the three classes has 50 samples each.) </li>
 <li> For each class, compute the sample mean $\widehat{\mathbf{\mu}}_k\in\mathbb{R}^4$, $k=1,\ldots, 3$
  $$ \widehat{\mathbf{\mu}}_k = \frac{1}{N_k}\sum_{\mathbf{x}_n\in \mathcal{C}_k} \mathbf{x}_n, $$
  where $N_k=50$ is the number of samples for each class. 
    </li>
  <li> For each class, compute the sample covariance matrix $\widehat{\mathbf{\Sigma}}_k\in\mathbb{R}^{4\times 4}$, $k=1,\ldots, 3$ 
    $$ \widehat{\mathbf{\Sigma}}_k = \frac{1}{N_k}\sum_{\mathbf{x}_n\in \mathcal{C}_k} \left(\mathbf{x}_n -\widehat{\mathbf{\mu}}_k\right)\left(\mathbf{x}_n -\widehat{\mathbf{\mu}}_k\right)^T $$
  </li>
  <li> Compute the joint covariance matrix 
  $$ \widehat{\mathbf{\Sigma}} = \sum_{k=1}^K \frac{N_k}{N}\widehat{\mathbf{\Sigma}}_k, $$
  where $N_k$ defines the number of samples of class $k$, and $N=150$ refers to the total number of samples.
    </li>
  <li> Compute the three class-specific discriminant functions 
    $$ a_k\left(\mathbf{x}\right) = \mathbf{w}_k^T \mathbf{x} + w_{k0}, $$
    using the previous results from task (b), and task (d), by
    $$ \mathbf{w}_k =\widehat{\mathbf{\Sigma}}^{-1} \widehat{\mathbf{\mu}}_k $$
    $$ w_{k0} = \frac{1}{2} \widehat{\mathbf{\mu}}_k^T \widehat{\mathbf{\Sigma}}^{-1} \widehat{\mathbf{\mu}}_k +\ln(\mathbb{P}(\mathcal{C}_k)), $$
    where $\mathbb{P}(\mathcal{C}_k) = \frac{N_k}{N}$.
    </li>
  <li> Assume each class is defined by a 4D Gaussian distribution $\mathcal{N}(\widehat{\mathbf{\mu}}_k,\widehat{\mathbf{\Sigma}})$. Get two new samples $\mathbf{x}_m^{(k)}\in\mathbb{R}^4$, $m=1,2$ for each class $k$.
	Hint: Check  https://numpy.org/doc/stable/reference/random/generated/numpy.random.multivariate_normal.html  </li>
  <li> Classify the new samples from the previous step (f) using the discriminant functions in step (e). This is done by computing $a_1(\mathbf{x}_m^{(k)})$, $a_2(\mathbf{x}_m^{(k)})$, and $a_3(\mathbf{x}_m^{(k)})$. Then assign the class $j$ by $j=\text{argmax}_{k} a_k$. <br/>
	Are the new samples $\mathbf{x}_m$ from step (f) correctly classified? 
	If not which classes are confused with one-another?
 </li>
</ol>
Additional non-mandatory task:
Select 2 of the 4 features and repeat steps (a)-(g) in the lower dimensional space. 


## Task 5: Theory
You are being handed a project where the task is to classify data samples into 2 classes, e.g. healthy vs. ill. 
Your colleague tells you excitedly that the model achieves 95\% accuracy. 
Upon further investigation you find that class 2 is always classified as class 1. 

How is this possible? 
Which metric(s) can be used instead of accuracy to capture this issue?
