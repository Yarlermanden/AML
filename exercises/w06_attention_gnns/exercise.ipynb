{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Bahdanau attention in encoder-decoder architecture using two RNNs for sequence to sequence learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Carefully go through the encoder and definitions\n",
    "- Which type of data (query, key, or values) is represented in the decoder's hidden states?\n",
    "    - the all-layer hidden states of the encoder at final time step\n",
    "        - Meaning decoder is initialized with last hidden states of encoder\n",
    "        - query = previous hidden state of decoder\n",
    "        - keys and values = previous hidden states of encoder\n",
    "    - The hidden states of the decoder is used as query of the attention at each step\n",
    "- Which type of data is represented in the encoder's hidden states?\n",
    "    - In this example, we don't use query, ky and values in the encoder - it's simplified\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Modify the experiment to replace the additive attention scoring function with scaled dot-product\n",
    "- How does it influence training efficiency?\n",
    "    - We use scaled dot-product attention to make it more efficient instead of additive attention\n",
    "    - Decreased traning time from 60sec to 50sec and decreased the number of parameters, as we do not need to learn the shared variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c. Replace GRU with LSTM in the experiment\n",
    "- How does this change performance w.r.t. the number of (trainable) parameters?\n",
    "    - LSTM converged slower and also took longer to train\n",
    "    - Resulted in the same performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### d. Expand the attention visualization with some labels and experiment with some other examples for *engs* and *frags*\n",
    "- Is the attention distribution sensible, despite the small data, and short training time?\n",
    "    - No, it needs more dat and training time. It consistently mappes more to '.' and end of sentence than any other word\n",
    "- What mapping (most salient token) of input and output tokens did you find?\n",
    "    - '.' and end of sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: \n",
    "Exploratory and broad - solved in plenum discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Read on of the key papers on Attention\n",
    "- Which did I choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Browse through the vast resources on transformer models toget a feeling for the variants and use cases.\n",
    "- Which of the provided resources did I choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c. Focus on one (or several) guiding questions for the discussion:\n",
    "- What were the key findings in the evolutino of the attention score function?\n",
    "- What is the scalability of different transformer models in training and deployment?\n",
    "- What tasks have been solved well with transformer models, and where are the current key challenges?\n",
    "- What specific decoder and encoder designs are suggested for different natural language processing, audio, and vision tasks?\n",
    "- What could be a taxonomy of the many different (efficient) transformer architectures (w.r.t. the key properties)\n",
    "- What are key numeric and semantic metrics in the major benchmarks tests and leadersboards?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3:\n",
    "Convolution in graph neural networks - particular Graph Convolution Networks (GCNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Understand how we represent the task for the GCN and in particular, the node-to-node operators\n",
    "- What does the k-power of the adjacency matrix $\\mathbf{A}$ represent?\n",
    "    - Represents the edge between the nodes: whether two kids hang out outside the club\n",
    "        - one if the two nodes have a connection\n",
    "- I.e. what does the element $(A^k)_{ij}$ represent?\n",
    "    - $A_{ij}$ means the connection (edge) between i and j\n",
    "    - $(A^k)_{ij}$ means all the paths of length k-1 starting at node i and ending in node j. That is it counts the number of paths with that length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This (social network) is not euclidean\n",
    "    - Abitrary distance betwen nodes - with social network\n",
    "    - We can not easily measure the distance and the actual distance might not have a particular easy interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Now understand how to build the GCN model by using a Laplacian as filter\n",
    "- What would our matrices $\\mathbf{Z}$ (feature maps) and $\\mathbf{W}$ (weights) represent?\n",
    "    - Z: The learned freature representation of the node \n",
    "    - W: Feature transformation after propagation with neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c. As expected, the MLP can't learn much about the nodes that we didn't train on\n",
    "- why was the GCN able to achieve this?\n",
    "    - In MLP we only include what club each kid go to (the nodes)\n",
    "    - in GCN we include the whole graph and thereby also the spectrum of the graph\n",
    "        - We can have an arbitrary walk. In MLP its implicit, but we cannot learn it. Where here we have to learn it.\n",
    "\n",
    "\n",
    "Hint: recall that due to the multiplcation with the Laplacian, the node embeddings calculated by the GCN at each layer combine the previous layer features from neighboring nodes.\n",
    "When we back-propagated from our two labelled nodes, we also updated the model parameters to produce more meaningful embedding for their neighbors!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
