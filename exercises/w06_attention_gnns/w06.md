
# Exercise week 6

Welcome to the course Advanced Machine Learning! This is your sixth mandatory exercise which must be completed until next week, 11.10.2022, 14:00. By that time you must fill out the checklist on the LearnIt page to indicate which tasks you completed and volunteer to present.


## Task 1: 

In today's first task, we look into the Bahdanau attention in encoder-decoder architecture based on two RNNs for sequence to sequence learning, via 
https://colab.research.google.com/drive/1iiyqJTPudKD5jaZT8oBjbKTE5g3WkNeQ (alternatively: w06_attention_pytorch.ipynb on your local machine)

The key aim is to understand how attention is used to abstract the entire input sequence into a context variable and how to produce an output sequence from this. Again, the example notebook includes many steps, so take your time and read carefully. Also, focus on the questions below; otherwise, this might be way too much to take in.

<ol type ="a">
 <li>Carefully go through the encoder and definitions. 
  
  - Which type of data (query, key, or values) is represented in the decoders's hidden states?
  - Which type of data is represented in the encoder's hidden states?
 </li>
 <li>Modify the experiment to replace the additive attention scoring function with the scaled dot-product. 
  
  - How does it influence training efficiency?</li>
 <li>Replace GRU with LSTM in the experiment.
  
  - How does this change performance w.r.t. the number of (trainable) parameters?
 </li>
 <li>Expand the attention visualisation with some labels and experiment with some other examples for *engs* and *fras*. 

  - Is the attention distribution sensible, despite the small data, and short training time?
  - What mapping (most salient token) of input and output tokens did you find?
 </li>
</ol>



## Task 2:

This task is **important** purposefully exploratory and broad, in order to give you the chance to grasp some details in your interest. We will solve this task in class as a plenum discussion, where your fulfilment of the exercise indicates that you can contribute something to the discussion.

##### Material:

<ol type ="a">
 <li>Read one of the key papers on Attention.</li>
  <li>Browse through the vast resources on transformer models to get a feeling for the variants and use cases. Below is just a tip of the iceberg.</li> 

*Hint:* Be a bit careful with random internet blogs as the quality is not always good.

Vaswani et al. 2017, 'Attention is all you need':
- https://arxiv.org/abs/1706.03762
- https://github.com/tensorflow/tensor2tensor
- https://github.com/jadore801120/attention-is-all-you-need-pytorch

Distill.pub blog on attention:
- https://distill.pub/2016/augmented-rnns/#attentional-interfaces

Blog post on attention and transformers by Lilian Weng 
- https://lilianweng.github.io/posts/2018-06-24-attention/

Huggingface, corporate blog on transformer models:
- https://huggingface.co/docs/transformers/

Step between GLoVe and transformers by Peters et al. 2018:
- https://arxiv.org/abs/1802.05365
- https://github.com/flairNLP/flair (replicated by Humboldt University)

BERT:
- Devlin et al. + Google 2019+: https://arxiv.org/abs/2005.14165
- https://github.com/google-research/bert
- BERS QA with SQuAD browse learned examples: https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/

GPT (1, 2, 3) - Radford et al. + Open AI 2018+:
- https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf
- https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf
- https://github.com/openai/finetune-transformer-lm
- https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
- https://github.com/openai/gpt-2
- https://arxiv.org/abs/2005.14165
- https://github.com/openai/gpt-3
- Generate text with gpt2: https://huggingface.co/distilgpt2

T5:
- Raffel et al. 2020: https://www.jmlr.org/papers/v21/20-074
- https://github.com/google-research/text-to-text-transfer-transformer

NLP Leaderboards:
- https://sites.research.google/xtreme
- https://super.gluebenchmark.com/leaderboard/
</ol>

##### Discussion Questions (and feel free to bring up more):
<ol start="3" type ="a">
<li>Focus on one (or several) guiding questions for the discussion:

- What were the key findings in the evolution of the attention score function?
- What is the scalability of different transformer models in training and deployment?
- What tasks have been solved well with transformer models, and where are the current key challenges?
- What specific decoder and encoder designs are suggested for different natural language processing, audio, and vision tasks?
- What could be a taxonomy of the many different (efficient) transformer architectures (w.r.t. the key properties)?
- What are key numeric and semantic metrics in the major benchmarks tests and leaderboards?
</li></ol>

## Task 3:

In this task, we explore convolution in graph neural networks, in particular Graph Convolution Networks (GCNs), via
https://colab.research.google.com/drive/15hfWb4KKHzdRTX9GgD9st39H4Ucnfm0V (alternatively: w06_geometric_deep_learning_pytorch.ipynb on your local machine)

The goal is first, to get to know about extending convolutional filters from images (grids) to the polynomial case (graphs), and second, to experience the GCN implementation in torch.

<ol type ="a">
 <li>Understand how we represent the task for the GCN and in particular, the node-to-node operators.

  - What does the $k$-power of the adjacency matrix $A$ represent?
  - I.e. what does the element $(A^k)_{ij}$ represent? 

</li>
 <li>Now understand how to build the GCN model by using a Laplacian as filter.

  - What would our matrices $Z$ (feature maps) and $W$ (weights) represent? 

</li>
 <li>As expected, the MLP can't learn much about the nodes that we didn't train on.

  - Why was the GCN able to achieve this? 

   *Hint:* Recall that due to the multiplication with the Laplacian, the node embeddings calculated by the GCN at each layer combine the previous layer features from neighbouring nodes.

   When we back-propagated from our two labelled nodes, we also updated the model parameters to produce more meaningful embedding for their neighbours!  

</li>
</ol>

## Optional expert task 4:

Now we are getting a bit more meta (not in a literal sense): we look into the much bigger CORA dataset, where we work on text(ual descriptions) about Machine Learning subfields. 
via 
https://colab.research.google.com/drive/1RRdhEZLzy4XXqWMhuIgT8rNcTXCl-rRj (alternatively: w06_generalizing_convolution_GNN_pytorch.ipynb on your local machine)

The task might at first seem a bit abstract and difficult on its own, but focus on how the Graph Convolution Network can provide a different tool to capture dependencies in text sequences structurally.

<ol type ="a">
 <li>Understand how we represent the task for the MLP and the GCN and pay particular attention to the GraphConvolution module.

  - Did you notice anything strange in the forward pass? 
  - Which is the shape of the adj matrix? 
  - Which is the shape of the output? 

</li>
</ol>
